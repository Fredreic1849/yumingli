<!doctype html><html lang=zh><head><meta name=generator content="Hugo 0.147.8"><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>李聿明 (Yuming Li)</title><meta name=description content="我是北京大学的软件工程硕士生。我的研究兴趣在于视频生成模型的强化学习后训练、生成加速技术、统一多模态生成模型，以及融合视觉-语言-动作（VLA）的具身世界模型。我目前正在申请生成模型和具身智能领域的博士项目。"><meta name=author content="李聿明 (Yuming Li)"><meta property="og:type" content="website"><meta property="og:url" content="https://Fredreic1849.github.io/zero-academic-page-yuming/zh/"><meta property="og:title" content="李聿明 (Yuming Li)"><meta property="og:description" content="我是北京大学的软件工程硕士生。我的研究兴趣在于视频生成模型的强化学习后训练、生成加速技术、统一多模态生成模型，以及融合视觉-语言-动作（VLA）的具身世界模型。我目前正在申请生成模型和具身智能领域的博士项目。"><meta property="og:image" content="/zero-academic-page-yuming/images/profile.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="李聿明 (Yuming Li)"><meta name=twitter:description content="我是北京大学的软件工程硕士生。我的研究兴趣在于视频生成模型的强化学习后训练、生成加速技术、统一多模态生成模型，以及融合视觉-语言-动作（VLA）的具身世界模型。我目前正在申请生成模型和具身智能领域的博士项目。"><meta name=twitter:image content="/zero-academic-page-yuming/images/profile.png"><link rel=canonical href=https://Fredreic1849.github.io/zero-academic-page-yuming/zh/><link rel=icon type=image/x-icon href=/favicon.ico><link rel=stylesheet href=/zero-academic-page-yuming/css/main.min.css><link rel=stylesheet href=/zero-academic-page-yuming/css/custom.min.css><script src=/zero-academic-page-yuming/js/main.min.js defer></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css><link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel=stylesheet></head><body class=light-mode><header class=navbar><div class=navbar-start><a href=https://Fredreic1849.github.io/zero-academic-page-yuming/zh/>李聿明 (Yuming Li)</a></div><div class="navbar-center hide-in-mobile"><div class=nav-links><div class=nav-item><a href=#about class=nav-link>关于我</a></div><div class=nav-item><a href=#education class=nav-link>教育背景</a></div><div class=nav-item><a href=#publications class=nav-link>发表论文</a></div><div class=nav-item><a href=#experience class=nav-link>个人经历</a></div><div class=nav-item><a href=yumingli/files/cv_liyuming.pdf class=nav-link>简历</a></div></div></div><div class=navbar-end><div id=i18n-selector><i class="fas fa-language"></i>
<select onchange="window.location.href=this.selectedOptions[0].value"><option value=https://Fredreic1849.github.io/zero-academic-page-yuming/>English</option><option value=https://Fredreic1849.github.io/zero-academic-page-yuming/zh/ selected>中文</option></select></div><div class=theme-toggle><button id=theme-toggle-btn aria-label="Toggle dark mode">
<i class="fas fa-moon dark-icon"></i>
<i class="fas fa-sun light-icon"></i></button></div><button type=button id=toggle-navbar-button class="toggle-navbar-button hide-in-desktop" aria-label="Toggle Navbar" aria-expanded=false aria-controls=navscreen onclick=toggleNavbar()>
<span><span class=top></span>
<span class=middle></span>
<span class=bottom></span></span></button></div></header><div id=navscreen class="navscreen hide-in-desktop"><div class=nav-screen-container><div class=nav-screen-links><div class=nav-links-item><a href=#about class=nav-link>关于我</a></div><div class=nav-links-item><a href=#education class=nav-link>教育背景</a></div><div class=nav-links-item><a href=#publications class=nav-link>发表论文</a></div><div class=nav-links-item><a href=#experience class=nav-link>个人经历</a></div><div class=nav-links-item><a href=yumingli/files/cv_liyuming.pdf class=nav-link>简历</a></div></div></div></div><div class=container><aside class="profile-card hide-in-mobile"><div class=profile-container><div class=profile-image><img src=/zero-academic-page-yuming/images/profile.png alt="李聿明 (Yuming Li)" class=rounded-avatar></div><div class=profile-name><h2>李聿明 (Yuming Li)</h2></div><div class=profile-description><p></p></div><div class=profile-quote><p></p></div><div class=social-links><a href=https://github.com/Fredreic1849 target=_blank rel="noopener noreferrer" class=social-icon><i class="fab fa-github"></i>
</a><a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=LQ-kxSEAAAAJ" target=_blank rel="noopener noreferrer" class=social-icon><i class="fab fa-google-scholar"></i>
</a><a href=mailto:2301210310@stu.pku.edu.cn target=_blank rel="noopener noreferrer" class=social-icon><i class="fas fa-envelope"></i>
</a><a href=https://www.linkedin.com/in/yuming-li-192700363 target=_blank rel="noopener noreferrer" class=social-icon><i class="fab fa-linkedin"></i></a></div></div></aside><main class=main-content><article class="single-page home-page"><h1 id=about>关于我</h1><p>我是北京大学的软件工程硕士生。我的研究兴趣在于视频生成模型的强化学习后训练、生成加速技术、统一多模态生成模型，以及融合视觉-语言-动作（VLA）的具身世界模型。我目前正在申请生成模型和具身智能领域的博士项目。</p><h1 id=education>📖 教育背景</h1><div class=education-entry><div class=header-left><div class=school>Peking University</div><div class=degree>Master's in Software Engineering</div></div><div class=header-right><div class=location>Beijing, China</div><div class=date>2023 – Present</div></div></div><div class=education-entry><div class=header-left><div class=school>Northwestern Polytechnical University</div><div class=degree>Bachelor's in Computer Science and Technology</div></div><div class=header-right><div class=location>Xi'an, China</div><div class=date>2019 – 2023</div></div></div><h1 id=publications>📝 发表论文</h1><ul><li><p><em><strong>A-ToMe: Adaptive Token Merge for Diffusion Models</strong></em><br><strong>Yuming Li</strong>*, M. Lu, Z. Li, X. Chi, Q. She, S. Zhang<br><em>arXiv</em>, 2025.</p></li><li><p><em><strong>ASGDiffusion: Parallel High-Resolution Generation with Asynchronous Structure Guidance</strong></em><br><strong>Yuming Li</strong>*, P. Jia, D. Hong, Y. Jia, Q. She, R. Zhao, M. Lu, S. Zhang<br><em>arXiv</em>, 2025.</p></li><li><p><em><strong>FastInit: Fast Noise Initialization for Temporally Consistent Video Generation</strong></em><br>C. Bai, <strong>Yuming Li</strong>*, M. Lu, S. Zhang<br><em>arXiv</em>, 2025.</p></li><li><p><em><strong>ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy</strong></em><br>G. Li, Y. Gao, <strong>Yuming Li</strong>, Y. Wu<br><em>arXiv</em>, 2025.</p></li><li><p><em><strong>ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance</strong></em><br>Y. Li, X. Wei, X. Chi, <strong>Yuming Li</strong>, Z. Zhao, H. Wang, N. Ma, M. Lu, S. Zhang<br><em>arXiv</em>, 2025.</p></li><li><p><em><strong>PiGW: A Plug-in Generative Watermarking Framework</strong></em><br>R. Ma, M. Guo, <strong>Yuming Li</strong>, H. Zhang, C. Ma, X. Xie, S. Zhang<br><em>arXiv</em>, 2025.</p></li></ul><h1 id=experience>💻 个人经历</h1><div class=experience-entry><div class=header><div class=header-left><div class=company>X-Humanoid</div><div class=role>Research Intern</div></div><div class=header-right><div class=location>Beijing, China</div><div class=date>Feb 2025 – Dec 2025</div></div></div><div class=description><ul><li>Focused on fine-tuning Wan2.1-based video diffusion models for scene-level world simulation and post-trained reinforcement learning agents on the generated environments.</li><li>Supported downstream video-to-action tasks through integration with Multimodal Large Language Models (MLLMs) after world model construction.</li></ul></div></div><div class=experience-entry><div class=header><div class=header-left><div class=company>ByteDance</div><div class=role>Research Intern, Commercialization Department</div></div><div class=header-right><div class=location>Beijing, China</div><div class=date>Feb 2025 – June 2025</div></div></div><div class=description><ul><li>Trained a unified <em>Multimodal Understanding & Generation</em> large model; optimized its cross‑modal tokenizer, then fine‑tuned the model for image editing and introduced tabular tokens to boost classification performance.</li><li>Optimized the initial noise for video generation, enhancing the temporal consistency and quality of generated videos.</li></ul></div></div><div class=experience-entry><div class=header><div class=header-left><div class=company>Tencent</div><div class=role>Research Intern, RoboticsX</div></div><div class=header-right><div class=location>Shenzhen, China</div><div class=date>Jun 2024 – Feb 2025</div></div></div><div class=description><ul><li>Investigated visual‑language‑action (VLA) models for robotic manipulation; reproduced the Pi‑0 model and diffusion‑policy.</li><li>Built a dynamic‑model‑based dual‑arm system for cloth folding, leveraging 3D point‑cloud state estimation to achieve precise manipulation of flexible objects.</li></ul></div></div><div class=experience-entry><div class=header><div class=header-left><div class=company>Phigent Robotics</div><div class=role>Research Intern</div></div><div class=header-right><div class=location>Beijing, China</div><div class=date>Dec 2023 – Jun 2024</div></div></div><div class=description><ul><li>Focused on efficient high-resolution image generation using low-memory techniques.</li></ul></div></div><div class=experience-entry><div class=header><div class=header-left><div class=company>Peking University HMI Lab</div><div class=role>Research Assistant</div></div><div class=header-right><div class=location>Beijing, China</div><div class=date>Sep 2023 – Present</div></div></div></div></article></main></div><footer class=footer></footer></body></html>