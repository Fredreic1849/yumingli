---
title: "Yuming Li"
description: >-
  I am a Master's student at Peking University, specializing in Software Engineering. My research interests lie in reinforcement learning post-training for video generative models, generative acceleration techniques, unified multimodal generative model, embodied world models with visual-language-action (VLA) integration. I am currently applying for Ph.D. programs in the fields of generative models and embodied AI.
---

# About Me {#about}

I am a Master's student at Peking University, specializing in Software Engineering. My research centers on generative models, with a focus on video generation, reinforcement learning post-training, unified multimodal generation and embodied AI. I am currently applying for Ph.D. programs in generative modeling and embodied intelligence.

# üìñ Education {#education}

<div class="education-entry">
    <div class="header-left">
        <div class="school">Peking University</div>
        <div class="degree">Master's in Software Engineering</div>
    </div>
    <div class="header-right">
        <div class="location">Beijing, China</div>
        <div class="date">2023 ‚Äì Present</div>
    </div>
</div>

<div class="education-entry">
    <div class="header-left">
        <div class="school">Northwestern Polytechnical University</div>
        <div class="degree">Bachelor's in Computer Science and Technology</div>
    </div>
    <div class="header-right">
        <div class="location">Xi'an, China</div>
        <div class="date">2019 ‚Äì 2023</div>
    </div>
</div>

# üìù Publications {#publications}

- ***A-ToMe: Adaptive Token Merge for Diffusion Models*** <br>
  **Yuming Li***, M. Lu, Z. Li, X. Chi, Q. She, S. Zhang <br>
  *arXiv*, 2025.

- ***ASGDiffusion: Parallel High-Resolution Generation with Asynchronous Structure Guidance*** <br>
  **Yuming Li***, P. Jia, D. Hong, Y. Jia, Q. She, R. Zhao, M. Lu, S. Zhang <br>
  *arXiv*, 2025.

- ***FastInit: Fast Noise Initialization for Temporally Consistent Video Generation*** <br>
  C. Bai, **Yuming Li***, M. Lu, S. Zhang <br>
  *arXiv*, 2025.

- ***ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy*** <br>
  G. Li, Y. Gao, **Yuming Li**, Y. Wu <br>
  *arXiv*, 2025.

- ***ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance*** <br>
  Y. Li, X. Wei, X. Chi, **Yuming Li**, Z. Zhao, H. Wang, N. Ma, M. Lu, S. Zhang <br>
  *arXiv*, 2025.

- ***PiGW: A Plug-in Generative Watermarking Framework*** <br>
  R. Ma, M. Guo, **Yuming Li**, H. Zhang, C. Ma, X. Xie, S. Zhang <br>
  *arXiv*, 2025.

# üíª Experience {#experience}

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">X-Humanoid</div>
      <div class="role">Research Intern</div>
    </div>
    <div class="header-right">
      <div class="location">Beijing, China</div>
      <div class="date">Feb 2025 ‚Äì Dec 2025</div>
    </div>
  </div>
  <div class="description">
    <ul>
      <li>Focused on fine-tuning Wan2.1-based video diffusion models for scene-level world simulation and post-trained reinforcement learning agents on the generated environments.</li>
      <li>Supported downstream video-to-action tasks through integration with Multimodal Large Language Models (MLLMs) after world model construction.</li>
    </ul>
  </div>
</div>

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">ByteDance</div>
      <div class="role">Research Intern, Commercialization Department</div>
    </div>
    <div class="header-right">
      <div class="location">Beijing, China</div>
      <div class="date">Feb 2025 ‚Äì June 2025</div>
    </div>
  </div>
  <div class="description">
    <ul>
      <li>Trained a unified <em>Multimodal Understanding &amp; Generation</em> large model; optimized its cross‚Äëmodal tokenizer, then fine‚Äëtuned the model for image editing and introduced tabular tokens to boost classification performance.</li>
      <li>Optimized the initial noise for video generation, enhancing the temporal consistency and quality of generated videos.</li>
    </ul>
  </div>
</div>

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">Tencent</div>
      <div class="role">Research Intern, RoboticsX</div>
    </div>
    <div class="header-right">
      <div class="location">Shenzhen, China</div>
      <div class="date">Jun 2024 ‚Äì Feb 2025</div>
    </div>
  </div>
  <div class="description">
    <ul>
      <li>Investigated visual‚Äëlanguage‚Äëaction (VLA) models for robotic manipulation; reproduced the Pi‚Äë0 model and diffusion‚Äëpolicy.</li>
      <li>Built a dynamic‚Äëmodel‚Äëbased dual‚Äëarm system for cloth folding, leveraging 3D point‚Äëcloud state estimation to achieve precise manipulation of flexible objects.</li>
    </ul>
  </div>
</div>

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">Phigent Robotics</div>
      <div class="role">Research Intern</div>
    </div>
    <div class="header-right">
      <div class="location">Beijing, China</div>
      <div class="date">Dec 2023 ‚Äì Jun 2024</div>
    </div>
  </div>
  <div class="description">
    <ul>
      <li>Focused on efficient high-resolution image generation using low-memory techniques.</li>
    </ul>
  </div>
</div>

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">Peking University HMI Lab</div>
      <div class="role">Research Assistant</div>
    </div>
    <div class="header-right">
      <div class="location">Beijing, China</div>
      <div class="date">Sep 2023 ‚Äì Present</div>
    </div>
  </div>
</div>