---
title: "æè¿æ˜ (Yuming Li)"
description: >-
  æˆ‘æ˜¯åŒ—äº¬å¤§å­¦çš„è½¯ä»¶å·¥ç¨‹ç¡•å£«ç”Ÿã€‚æˆ‘çš„ç ”ç©¶å…´è¶£åœ¨äºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒã€ç”ŸæˆåŠ é€ŸæŠ€æœ¯ã€ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠèåˆè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰çš„å…·èº«ä¸–ç•Œæ¨¡å‹ã€‚æˆ‘ç›®å‰æ­£åœ¨ç”³è¯·ç”Ÿæˆæ¨¡å‹å’Œå…·èº«æ™ºèƒ½é¢†åŸŸçš„åšå£«é¡¹ç›®ã€‚
---

# å…³äºæˆ‘ {#about}

æˆ‘æ˜¯åŒ—äº¬å¤§å­¦çš„è½¯ä»¶å·¥ç¨‹ç¡•å£«ç”Ÿã€‚æˆ‘çš„ç ”ç©¶å…´è¶£åœ¨äºè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ åè®­ç»ƒã€ç”ŸæˆåŠ é€ŸæŠ€æœ¯ã€ç»Ÿä¸€å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œä»¥åŠèåˆè§†è§‰-è¯­è¨€-åŠ¨ä½œï¼ˆVLAï¼‰çš„å…·èº«ä¸–ç•Œæ¨¡å‹ã€‚æˆ‘ç›®å‰æ­£åœ¨ç”³è¯·ç”Ÿæˆæ¨¡å‹å’Œå…·èº«æ™ºèƒ½é¢†åŸŸçš„åšå£«é¡¹ç›®ã€‚

# ğŸ“– æ•™è‚²èƒŒæ™¯ {#education}

<div class="education-entry">
    <div class="header-left">
        <div class="school">Peking University</div>
        <div class="degree">Master's in Software Engineering</div>
    </div>
    <div class="header-right">
        <div class="location">Beijing, China</div>
        <div class="date">2023 â€“ Present</div>
    </div>
</div>

<div class="education-entry">
    <div class="header-left">
        <div class="school">Northwestern Polytechnical University</div>
        <div class="degree">Bachelor's in Computer Science and Technology</div>
    </div>
    <div class="header-right">
        <div class="location">Xi'an, China</div>
        <div class="date">2019 â€“ 2023</div>
    </div>
</div>

# ğŸ“ å‘è¡¨è®ºæ–‡ {#publications}

- ***A-ToMe: Adaptive Token Merge for Diffusion Models*** <br>
  **Yuming Li***, M. Lu, Z. Li, X. Chi, Q. She, S. Zhang <br>
  *arXiv*, 2025.

- ***ASGDiffusion: Parallel High-Resolution Generation with Asynchronous Structure Guidance*** <br>
  **Yuming Li***, P. Jia, D. Hong, Y. Jia, Q. She, R. Zhao, M. Lu, S. Zhang <br>
  *arXiv*, 2025.

- ***FastInit: Fast Noise Initialization for Temporally Consistent Video Generation*** <br>
  C. Bai, **Yuming Li***, M. Lu, S. Zhang <br>
  *arXiv*, 2025.

- ***ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy*** <br>
  G. Li, Y. Gao, **Yuming Li**, Y. Wu <br>
  *arXiv*, 2025.

- ***ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance*** <br>
  Y. Li, X. Wei, X. Chi, **Yuming Li**, Z. Zhao, H. Wang, N. Ma, M. Lu, S. Zhang <br>
  *arXiv*, 2025.

- ***PiGW: A Plug-in Generative Watermarking Framework*** <br>
  R. Ma, M. Guo, **Yuming Li**, H. Zhang, C. Ma, X. Xie, S. Zhang <br>
  *arXiv*, 2025.

# ğŸ’» ä¸ªäººç»å† {#experience}

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">X-Humanoid</div>
      <div class="role">Research Intern</div>
    </div>
    <div class="header-right">
      <div class="location">Beijing, China</div>
      <div class="date">Feb 2025 â€“ Dec 2025</div>
    </div>
  </div>
  <div class="description">
    <ul>
      <li>Focused on fine-tuning Wan2.1-based video diffusion models for scene-level world simulation and post-trained reinforcement learning agents on the generated environments.</li>
      <li>Supported downstream video-to-action tasks through integration with Multimodal Large Language Models (MLLMs) after world model construction.</li>
    </ul>
  </div>
</div>

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">ByteDance</div>
      <div class="role">Research Intern, Commercialization Department</div>
    </div>
    <div class="header-right">
      <div class="location">Beijing, China</div>
      <div class="date">Feb 2025 â€“ June 2025</div>
    </div>
  </div>
  <div class="description">
    <ul>
      <li>Trained a unified <em>Multimodal Understanding &amp; Generation</em> large model; optimized its crossâ€‘modal tokenizer, then fineâ€‘tuned the model for image editing and introduced tabular tokens to boost classification performance.</li>
      <li>Optimized the initial noise for video generation, enhancing the temporal consistency and quality of generated videos.</li>
    </ul>
  </div>
</div>

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">Tencent</div>
      <div class="role">Research Intern, RoboticsX</div>
    </div>
    <div class="header-right">
      <div class="location">Shenzhen, China</div>
      <div class="date">Jun 2024 â€“ Feb 2025</div>
    </div>
  </div>
  <div class="description">
    <ul>
      <li>Investigated visualâ€‘languageâ€‘action (VLA) models for robotic manipulation; reproduced the Piâ€‘0 model and diffusionâ€‘policy.</li>
      <li>Built a dynamicâ€‘modelâ€‘based dualâ€‘arm system for cloth folding, leveraging 3D pointâ€‘cloud state estimation to achieve precise manipulation of flexible objects.</li>
    </ul>
  </div>
</div>

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">Phigent Robotics</div>
      <div class="role">Research Intern</div>
    </div>
    <div class="header-right">
      <div class="location">Beijing, China</div>
      <div class="date">Dec 2023 â€“ Jun 2024</div>
    </div>
  </div>
  <div class="description">
    <ul>
      <li>Focused on efficient high-resolution image generation using low-memory techniques.</li>
    </ul>
  </div>
</div>

<div class="experience-entry">
  <div class="header">
    <div class="header-left">
      <div class="company">Peking University HMI Lab</div>
      <div class="role">Research Assistant</div>
    </div>
    <div class="header-right">
      <div class="location">Beijing, China</div>
      <div class="date">Sep 2023 â€“ Present</div>
    </div>
  </div>
</div>